route:
  # The labels by which incoming alerts are grouped together. For example,
  # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
  # be batched into a single group.
  #
  # severity - https://docs.gitlab.com/ee/operations/metrics/alerts.html#external-prometheus-instances
  #   Critical: critical, s1, p1, emergency, fatal, or any value not in this list
  # High: high, s2, p2, major, page
  # Medium: medium, s3, p3, error, alert
  # Low: low, s4, p4, warn, warning
  # Info: info, s5, p5, debug, information, notice
  # Alerts Dashboard: https://github.com/alerta/prometheus-config
  # To aggregate by all possible labels use '...' as the sole label name.
  # This effectively disables aggregation entirely, passing through all
  # alerts as-is. This is unlikely to be what you want, unless you have
  # a very low alert volume or your upstream notification system performs
  # its own grouping. Example: group_by: [...]
  group_by: ['alertname', 'job']

  # When a new group of alerts is created by an incoming alert, wait at
  # least 'group_wait' to send the initial notification.
  # This way ensures that you get multiple alerts for the same group that start
  # firing shortly after another are batched together on the first
  # notification.
  group_wait: 30s

  # When the first notification was sent, wait 'group_interval' to send a batch
  # of new alerts that started firing for that group.
  group_interval: 5m

  # If an alert has successfully been sent, wait 'repeat_interval' to
  # resend them.
  repeat_interval: 3h

  # A default receiver - Alertsnitch? https://gitlab.com/yakshaving.art/alertsnitch
  receiver: alertsnitch

  routes:

    - receiver: 'l1critical'
      continue: true
      matchers:
      - severity=~"critical"

  {% if slack_webhook_url is defined %}

    - receiver: 'sudo-slack'
      continue: true
      matchers:
      - severity=~"critical|page"

  {% endif %}



receivers:
  - name: alertsnitch
    webhook_configs:
      - url: http://alertsnitch:9567/webhook

  {% if alert_handler_webhook_url is defined %}

  - name: 'l1critical'
    webhook_configs:
    - url: {{ alert_handler_webhook_url }}
      send_resolved: true
  {% endif %}

  {% if slack_webhook_url is defined %}

  - name: 'sudo-slack'
    slack_configs:
      - send_resolved: true
        username: 'Prometheus'
        channel: {{ slack_channel_name }}
        api_url: '{{ slack_webhook_url }}'
        title: |-
          [{{ '{{' }} .Status | toUpper }}{{ '{{' }} if eq .Status "firing" }}:{{ '{{' }} .Alerts.Firing | len }}{{ '{{' }} end }}] {{ '{{' }} .CommonLabels.alertname }} for {{ '{{' }} .CommonLabels.job }}
          {{ '{{' }}- if gt (len .CommonLabels) (len .GroupLabels) -}}
            {{ '{{' }}" "}}(
            {{ '{{' }}- with .CommonLabels.Remove .GroupLabels.Names }}
              {{ '{{' }}- range $index, $label := .SortedPairs -}}
                {{ '{{' }} if $index }}, {{ '{{' }} end }}
                {{ '{{' }}- $label.Name }}="{{ '{{' }} $label.Value -}}"
              {{ '{{' }}- end }}
            {{ '{{' }}- end -}}
            )
          {{ '{{' }}- end }}
        text: >-
          {{ '{{' }} range .Alerts -}}
          *Alert:* {{ '{{' }} .Annotations.title }}{{ '{{' }} if .Labels.severity }} - `{{ '{{' }} .Labels.severity }}`{{ '{{' }} end }}

          *Description:* {{ '{{' }} .Annotations.description }}

          *Details:*
            {{ '{{' }} range .Labels.SortedPairs }} â€¢ *{{ '{{' }} .Name }}:* `{{ '{{' }} .Value }}`
            {{ '{{' }} end }}
          {{ '{{' }} end }}
  {% endif %}

# Inhibition rules allow to mute a set of alerts given that another alert is
# firing.
# We use this to mute any warning-level notifications if the same alert is
# already critical.
inhibit_rules:
  - source_matchers: [ severity="critical" ]
    target_matchers: [ severity="warning" ]
    # Apply inhibition if the alertname is the same.
    # CAUTION:
    #   If all label names listed in `equal` are missing
    #   from both the source and target alerts,
    #   the inhibition rule will apply!
    equal: [ alertname, cluster, service ]
  - source_matchers: [ severity="warning" ]
    target_matchers: [ severity="info" ]
    # Apply inhibition if the alertname is the same.
    # CAUTION:
    #   If all label names listed in `equal` are missing
    #   from both the source and target alerts,
    #   the inhibition rule will apply!
    equal: [ alertname, cluster, service ]
  - source_matchers: [ severity="critical" ]
    target_matchers: [ severity="warning" ]
    # Apply inhibition if the alertname is the same.
    # CAUTION:
    #   If all label names listed in `equal` are missing
    #   from both the source and target alerts,
    #   the inhibition rule will apply!
    equal: [ alertname, instance, job ]
  - source_matchers: [ severity="warning" ]
    target_matchers: [ severity="info" ]
    # Apply inhibition if the alertname is the same.
    # CAUTION:
    #   If all label names listed in `equal` are missing
    #   from both the source and target alerts,
    #   the inhibition rule will apply!
    equal: [ alertname, instance, job ]
